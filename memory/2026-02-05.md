
## Sub-Agent Context System Built (00:24 UTC)

**Problem:** Sub-agents were hitting 200k token ceiling. One failed with 190k input + 34k max_tokens > 200k limit.

**Solution:** Lean, role-based context injection system.

**What we built:**
- `subagents/personas/` â€” Role personas (dev, qa, researcher, writer, spec) ~200 tokens each
- `subagents/guidelines.md` â€” Shared rules (delegation, db access)
- `subagents/context-builder.js` â€” Assembles lean prompts
- `tools/spawn.js` â€” CLI to spawn role-based sub-agents

**Result:** 190k â†’ ~2k tokens (99% reduction)

**Bugs fixed:**
- Circular dependency in embeddings.js (lazy import)
- Missing default model in searchMemoryByEmbedding
- Threshold too strict (0.7 â†’ 0.4)

**Key insight:** Sub-agents are like junior employees â€” give them the task and a reference manual, not your entire brain dump.

**Design decision:** I did this myself (not delegated) because it was 80% design/20% typing, and foundational infrastructure that affects all future sub-agents.

## Context Optimization Progress (00:30 UTC)

### âœ… #1 DONE: HEARTBEAT.md Conditional
- Moved to `config/HEARTBEAT.md` (no longer auto-injected)
- Saves ~1,245 tokens per message
- Heartbeat still works: reads from new location when triggered

### ðŸ”¬ #2 IN PROGRESS: MEMORY.md Semantic Search
**Current state:** MEMORY.md is injected wholesale (~1,824 tokens)
**Proposed:** Replace with semantic search at conversation start

**Challenge:** Requires OpenClaw config/code changes to:
1. NOT inject MEMORY.md 
2. At session start, run semantic search on first message
3. Inject top N relevant memories instead

**Options:**
a) Feature request to OpenClaw project
b) Rename MEMORY.md to not auto-inject, add retrieval to heartbeat/cron
c) Accept current state (it's only ~2k tokens)

**Decision:** Park for now. Sub-agent context wins are bigger (99% reduction). Main session optimization can wait.

## Research Pattern Locked In (00:40 UTC)

Jason approved this pattern for research tasks:

1. **Check existing docs first** â€” `docs/`, `memory/`, past work
2. **If gaps exist** â†’ delegate to Gemini via `tools/research.js`
3. **Review output** â†’ synthesize and deliver

**Rule:** Opus plans and judges. Cheap models fetch and summarize. No "quick" exceptions.

## Model Routing Audit & Upgrades (00:45-01:00 UTC)

### Tools Audited & Fixed
- `research.js` â€” fixed to use OpenRouter (was direct Gemini, hit rate limits)
- `reddit-pulse.js` â€” fixed to use OpenRouter (was direct Gemini)
- Added Brave Search integration to `research.js` (`-s` flag for search-first workflow)

### Model Upgrade: Gemini 2.0 â†’ 2.5-flash-lite
Ran comparison tests with `tools/model-compare.js`:

**2.0-flash vs 2.5-flash-lite:**
- Lite is 32% faster (732ms vs 1085ms avg)
- Same cost ($0.10/M in, $0.40/M out)
- Quality: comparable, lite slightly better at summarization

**2.5-flash-lite vs 2.5-flash (full):**
- Lite is 11% faster
- Full costs 6x more on output ($2.50/M vs $0.40/M)
- Quality: full is marginally better at summarization, not 6x better

**Decision:** Upgraded all tools to `gemini-2.5-flash-lite`. Parked full 2.5-flash for future when we track summarization volume.

### Embeddings Simplified
- Removed Gemini embedding fallback
- Now OpenAI-only: `text-embedding-3-small` ($0.02/M tokens)
- Cleaner code, no silent fallbacks

### Tools Updated
| Tool | Model |
|------|-------|
| research.js | gemini-2.5-flash-lite |
| reddit-pulse.js | gemini-2.5-flash-lite |
| gemini.js | gemini-2.5-flash-lite |
| health.js | gemini-2.5-flash-lite |
| lib/router.js | gemini-2.5-flash-lite |
| lib/embeddings.js | OpenAI text-embedding-3-small |
