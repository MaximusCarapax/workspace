
## Sub-Agent Context System Built (00:24 UTC)

**Problem:** Sub-agents were hitting 200k token ceiling. One failed with 190k input + 34k max_tokens > 200k limit.

**Solution:** Lean, role-based context injection system.

**What we built:**
- `subagents/personas/` â€” Role personas (dev, qa, researcher, writer, spec) ~200 tokens each
- `subagents/guidelines.md` â€” Shared rules (delegation, db access)
- `subagents/context-builder.js` â€” Assembles lean prompts
- `tools/spawn.js` â€” CLI to spawn role-based sub-agents

**Result:** 190k â†’ ~2k tokens (99% reduction)

**Bugs fixed:**
- Circular dependency in embeddings.js (lazy import)
- Missing default model in searchMemoryByEmbedding
- Threshold too strict (0.7 â†’ 0.4)

**Key insight:** Sub-agents are like junior employees â€” give them the task and a reference manual, not your entire brain dump.

**Design decision:** I did this myself (not delegated) because it was 80% design/20% typing, and foundational infrastructure that affects all future sub-agents.

## Context Optimization Progress (00:30 UTC)

### âœ… #1 DONE: HEARTBEAT.md Conditional
- Moved to `config/HEARTBEAT.md` (no longer auto-injected)
- Saves ~1,245 tokens per message
- Heartbeat still works: reads from new location when triggered

### ðŸ”¬ #2 IN PROGRESS: MEMORY.md Semantic Search
**Current state:** MEMORY.md is injected wholesale (~1,824 tokens)
**Proposed:** Replace with semantic search at conversation start

**Challenge:** Requires OpenClaw config/code changes to:
1. NOT inject MEMORY.md 
2. At session start, run semantic search on first message
3. Inject top N relevant memories instead

**Options:**
a) Feature request to OpenClaw project
b) Rename MEMORY.md to not auto-inject, add retrieval to heartbeat/cron
c) Accept current state (it's only ~2k tokens)

**Decision:** Park for now. Sub-agent context wins are bigger (99% reduction). Main session optimization can wait.

## Research Pattern Locked In (00:40 UTC)

Jason approved this pattern for research tasks:

1. **Check existing docs first** â€” `docs/`, `memory/`, past work
2. **If gaps exist** â†’ delegate to Gemini via `tools/research.js`
3. **Review output** â†’ synthesize and deliver

**Rule:** Opus plans and judges. Cheap models fetch and summarize. No "quick" exceptions.

## Model Routing Audit & Upgrades (00:45-01:00 UTC)

### Tools Audited & Fixed
- `research.js` â€” fixed to use OpenRouter (was direct Gemini, hit rate limits)
- `reddit-pulse.js` â€” fixed to use OpenRouter (was direct Gemini)
- Added Brave Search integration to `research.js` (`-s` flag for search-first workflow)

### Model Upgrade: Gemini 2.0 â†’ 2.5-flash-lite
Ran comparison tests with `tools/model-compare.js`:

**2.0-flash vs 2.5-flash-lite:**
- Lite is 32% faster (732ms vs 1085ms avg)
- Same cost ($0.10/M in, $0.40/M out)
- Quality: comparable, lite slightly better at summarization

**2.5-flash-lite vs 2.5-flash (full):**
- Lite is 11% faster
- Full costs 6x more on output ($2.50/M vs $0.40/M)
- Quality: full is marginally better at summarization, not 6x better

**Decision:** Upgraded all tools to `gemini-2.5-flash-lite`. Parked full 2.5-flash for future when we track summarization volume.

### Embeddings Simplified
- Removed Gemini embedding fallback
- Now OpenAI-only: `text-embedding-3-small` ($0.02/M tokens)
- Cleaner code, no silent fallbacks

### Tools Updated
| Tool | Model |
|------|-------|
| research.js | gemini-2.5-flash-lite |
| reddit-pulse.js | gemini-2.5-flash-lite |
| gemini.js | gemini-2.5-flash-lite |
| health.js | gemini-2.5-flash-lite |
| lib/router.js | gemini-2.5-flash-lite |
| lib/embeddings.js | OpenAI text-embedding-3-small |

## Session Continued (00:32 - 01:40 UTC)

### Research Pattern Formalized
- Check existing docs first â†’ delegate gaps to Gemini â†’ review & deliver
- Fixed `tools/research.js` to use OpenRouter (was hitting direct Gemini rate limits)
- Added Brave Search integration (`-s` flag) for search-first research

### Model Audit & Upgrade
- Audited all tools for model consistency
- Fixed `tools/reddit-pulse.js` â€” was using direct Gemini, now OpenRouter
- Upgraded all tools from `gemini-2.0-flash` to `gemini-2.5-flash-lite`
- Ran comparison tests: 2.5-flash-lite is 32% faster, same cost, comparable quality
- Parked full 2.5-flash ($2.50/M output) for future when we need extra quality

### Embeddings Simplified
- Removed Gemini fallback from `lib/embeddings.js`
- Now OpenAI-only: `text-embedding-3-small` ($0.02/M)

### Alignment Checklist Created
- `docs/ALIGNMENT_CHECKLIST.md` â€” when to run, what to check
- Added reference in AGENTS.md
- Triggers: new tool, new persona, model change, major refactor

### Sub-Agent Personas Built
- Created 4 personas: researcher, builder, writer, reviewer
- All tested and working
- Reviewer found 3 real bugs in research.js (fixed)

### Maintenance Cron Jobs
- Moved cost sync & health checks from heartbeat to dedicated cron jobs
- Cost Sync: every 4 hours
- Health Check: every 6 hours
- Slimmed HEARTBEAT.md to focus on catch-up tasks only

### Content Pipeline v2 (BROKEN)
- Wrote spec: `specs/content-pipeline-v2.md`
- Spawned 4 builders in parallel â€” BAD IDEA
- All 4 edited `tools/content.js` simultaneously, created merge conflict mess
- kanban command added but broken (loadData not defined)
- **TODO:** Revert to before builders, redo one feature at a time

### Lessons Learned
1. **Don't parallel-build on same file** â€” builders will conflict
2. **Sequential builds for shared files** â€” or clearly partition work
3. **Cost sync needs to be cron** â€” heartbeat only runs when idle
4. **2.5-flash-lite is the sweet spot** â€” fast, cheap, good enough
